---
title: "final"
author: "Jack"
date: "2022/5/27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Lec7. MC inference
```{r}
m <- 1000
g <- numeric(m)
for (i in 1:m) {
  x <- rnorm(2)
  g[i] <- abs(x[1] - x[2]) 
}
est <- mean(g)

# formula for the mc se
se <- sqrt(sum((g-mean(g))^2)) / m
se
```

####Confidence Interval: 
For the variance in the normal  (0, UCL) $UCL=(n-1)S/x^2_{\alpha, n-1}$
```{r}
n <- 20
alpha <- .05
x <- rnorm(n, mean=0, sd=2)
UCL <- (n-1)*var(x) / qchisq(alpha, df=n-1)
print(UCL)
```


For the mean($\mu$) in the normal (LCL, UCL)  $(LCL, UCL) = \mu+\sigma*Z_{\alpha/2}/\sqrt n$
```{r}
n <- 20
alpha <- .05
mu <- 0 
std <- 2
x <- rnorm(n, mean=mu, sd=std)
LCL <- mean(x) + std * qnorm(alpha/2) / sqrt(n)
UCL <- mean(x) - std * qnorm(alpha/2) / sqrt(n)
print(c(LCL, UCL))
```


#### Confidence level: 
For the variance in normal:
```{r}
n <- 20
alpha <- .05

UCL <- replicate(1000, expr= {
  x <- rnorm(n, mean=0, sd=2)
  (n-1)*var(x) / qchisq(alpha, df=n-1)})
c(sum(UCL>4), mean(UCL>4))
```


For the mean in normal:
```{r}
m = 1000; n = 50; mu <- 30
set.seed(15)
interval <- replicate(m, expr = {
  x <- rnorm(n, mean=mu, sd=10)
  ucl <- mean(x) - 10 * qnorm(0.025)/sqrt(n)
  lcl <- mean(x) + 10 * qnorm(0.025)/sqrt(n)
  res <- c(lcl, ucl)
})
1- (sum(interval[1,]>30 | interval[2,] < 30)/m)

```

#### MC for the Hypothesis Tests
h0: mu=30, h1: mu!=30
```{r}
m = 10000; n = 50; mu <- 30
set.seed(13)
alpha <- 0.05
sigma <- 10
p <- numeric(m)
for (i in 1:m) {
  x <- rnorm(n, mu, sigma)
  ttest <- t.test(x, alternative = "two.sided", mu = mu)
  p[i] <- ttest$p.value
}
# compare \alpha
p_hat <- mean(p < alpha)

## formula for hypothesis se
se_hat <- sqrt(p_hat * (1-p_hat)/m)
c(p_hat, se_hat)
```

test for the skewness with the same idea as above.
```{r}
n <- 50
sk <- function(x) {
  m3 <- mean((x-mean(x))^3)
  m2 <- mean((x-mean(x))^2)
  return (m3 /m2^1.5)
}
skewness.c1 <- function(n, cv){
  m <- 10000
  sktests <- numeric(m)
  # mixture of two distribution
  for (i in 1:m) {
    x1 <- rnorm(n, 0, 1)
    x2 <- rnorm(n, 1, 3)
    u <- runif(n)
    p <- as.integer(u>0.4)
    x <- p*x1 + (1-p)*x2
    
    # compare the value
    sktests[i] <- as.integer(abs(sk(x)) >= cv)
  }
  p.reject <- mean(sktests)
  return(p.reject)
}

cv<-qnorm(0.975,0,sqrt(6*(n-2)/((n+1)*(n+3))))
print(skewness.c1(n, cv))
```


### Lec10. Jackknife and boostrap
Jackknife method
```{r}
n <- nrow(law)
theta_hat<-cor(law$LSAT,law$GPA)
theta_jack <- numeric(n)
# print(law$LSAT)

for (i in 1:n){
  theta_jack[i] <- cor(law$LSAT[-i], law$GPA[-i])
}

# bias and se for the jackknife
bias_jack <- (n - 1) * (mean(theta_jack) - theta_hat)
se_jack <- sqrt((n-1) * mean((theta_jack - mean(theta_jack))^2))
c(bias_jack, se_jack)
```

Boostrap method:
```{r}
n <- 10000
r <- nrow(law)
theta_hat<-cor(law$LSAT,law$GPA)
theta_boot <- numeric(n)
for (i in 1:n){
  s <- sample(1:r, size=r, replace = TRUE)
  theta_boot[i] <- cor(law$LSAT[s], law$GPA[s])
}
bias_boot <- mean(theta_boot - theta_hat)
se_boot <- sd(theta_boot)
c(bias_boot, se_boot)

```

another bootstrap method and for the confident interval
```{r}
library(boot)
data(law, package = "bootstrap")
boot.obj <- boot(law, R=2000, statistic = function(x, i){
  cor(x[i,1], x[i,2])
})

# confident interval
print(boot.ci(boot.obj, type=c("basic", "norm", "perc")))
```

Compute a 95% bootstrap t confidence interval for the ratio statistic 
```{r}
boot.t.ci <- function(x, B = 500, R = 100, level = .95, statistic){
  #compute the bootstrap t CI
  x <- as.matrix(x); 
  n <- nrow(x);
  stat <- numeric(B); 
  se <- numeric(B)
  for (b in 1:B) {
    j <- sample(1:n, size = n, replace= TRUE)
    y <- x[j, ]
    stat[b] <- statistic(y)
    se[b] <- boot.se(y, R = R, f=statistic)
  }
  stat0 <- statistic(x); 
  se0 <- sd(stat)
  t.stats <- (stat - stat0)/se
  alpha <- 1 - level
  Qt <- quantile(t.stats, c(alpha/2, 1-alpha/2), type = 1)
  names(Qt) <- rev(names(Qt))
  CI <- rev(stat0 - Qt * se0)
}

boot.se <- function(x, R, f) {
#local function to compute the bootstrap
#estimate of standard error for statistic f(x)
  x <- as.matrix(x); 
  m <- nrow(x)
  th <- replicate(R, expr = {
  i <- sample(1:m, size = m, replace = TRUE)
  f(x[i, ])
  })
  return(sd(th))
}

dat <- cbind(patch$y, patch$z)
stat <- function(dat){
mean(dat[, 1]) / mean(dat[, 2]) }
ci <- boot.t.ci(dat, statistic = stat, B=2000, R=200)
print(ci)
```

### Lec11. kernel density 
Sturges’ Rule to the plot histogram
```{r}
n <- 25
x <- rnorm(n)
# calc breaks according to Sturges’ Rule
nclass <- ceiling(1 + log2(n))
cwidth <- diff(range(x) / nclass)
breaks <- min(x) + cwidth * 0:nclass
h.default <- hist(x, freq = FALSE, xlab = "default", main = "hist: default")
z <- qnorm(ppoints(1000))
lines(z, dnorm(z))
h.sturges <- hist(x, breaks = breaks, freq = FALSE, main = "hist: Sturges")
lines(z, dnorm(z))
```

Scott’s Normal Reference Rule to the plot histogram
```{r}
library(MASS) #for geyser and truehist
waiting <- geyser$waiting
n <- length(waiting)
# rounding the constant in Scott’s rule
# and using sample standard deviation to estimate sigma
h <- 3.5 * sd(waiting) * n^(-1/3)
# number of classes is determined by the range and h
m <- min(waiting)
M <- max(waiting)
nclass <- ceiling((M - m) / h)
breaks <- m + h * 0:nclass
par(mfrow=c(1,3))
h.scott<-hist(waiting, breaks=breaks, freq=FALSE,main="")
truehist(waiting,nbins="Scott", x0=0,prob=TRUE,col=0)
hist(waiting, breaks = "scott", prob=TRUE,density=5)

```

Frequency polygon density estimate
```{r}
waiting <- geyser$waiting #in MASS
n <- length(waiting)
# freq poly bin width using normal ref rule
h <- 2.15 * sd(waiting) * n^(-1/5)
# calculate the sequence of breaks and histogram
br <- pretty(waiting, diff(range(waiting)) / h)
brplus <- c(min(br)-h, max(br)+h)
histg <- hist(waiting, breaks = br, freq = FALSE, main = "", xlim = brplus)

# add line
vx <- histg$mids #density est at vertices of polygon
vy <- histg$density
delta <- diff(vx)[1] # h after pretty is applied
k <- length(vx)
vx <- c(vx[1] - delta, vx, vx[k] + delta)
vy <- c(0, vy, 0)
# add the polygon to the histogram
polygon(vx, vy)
```

ASH density estimate:
```{r}
library(MASS)
waiting <- geyser$waiting
n <- length(waiting)
m <- 20
a <- min(waiting) - .5
b <- max(waiting) + .5
# h can be 2.576 * \sigma * n^(-1/5)
h <- 7.27037
delta <- h / m
#get the bin counts on the delta-width mesh.
br <- seq(a - delta*m, b + 2*delta*m, delta)
histg <- hist(waiting, breaks = br, plot = FALSE)
nk <- histg$counts
K <- abs((1-m):(m-1))

fhat <- function(x) {
  # locate the leftmost interval containing x
  i <- max(which(x > br))
  k <- (i - m + 1):(i + m - 1)
  # get the 2m-1 bin counts centered at x
  vk <- nk[k]
  sum((1 - K / m) * vk) / (n * h) #f.hat
}
# density can be computed at any points in range of data
z <- as.matrix(seq(a, b + h, .1))
f.ash <- apply(z, 1, fhat) 

br2 <- seq(a, b + h, h)
hist(waiting, breaks = br2, freq = FALSE,
main = "", ylim = c(0, max(f.ash)))
lines(z, f.ash, xlab = "waiting")

```

```{r}
plot(density(waiting))
```

different with different bandwidth 
```{r}
n <- length(precip)
h1 <- 1.06 * sd(precip) * n^(-1/5)
h2 <- .9 * min(c(IQR(precip)/1.34, sd(precip))) * n^(-1/5)
h0 <- bw.nrd0(precip)

par(mfrow = c(2, 2))
plot(density(precip)) #default Gaussian (h0)
plot(density(precip, bw = h1)) #Gaussian, bandwidth h1
plot(density(precip, bw = h2)) #Gaussian, bandwidth h2
plot(density(precip, kernel = "cosine"))
```

```{r}
d <- density(precip)
xnew <- seq(0, 70, 10)
approx(d$x, d$y, xout = xnew) #

fhat <- approxfun(d$x, d$y)
fhat(xnew)
```


```{r}
x <- rexp(1000, 1)
plot(density(x), xlim = c(-1, 6), ylim = c(0, 1), main="")
abline(v = 0)
# add the true density to compare
y <- seq(.001, 6, .01)
lines(y, dexp(y, 1), lty = 2)  # dash line
```
Reflection boundary technique
```{r}
xx <- c(x, -x)
g <- density(xx, bw = bw.nrd0(x))
a <- seq(0, 6, .01)
ghat <- approx(g$x, g$y, xout = a)
fhat <- 2 * ghat$y # density estimate along a
bw <- paste("Bandwidth = ", round(g$bw, 5))
plot(a, fhat, type="l", xlim=c(-1, 6), ylim=c(0, 1),
main = "", xlab = bw, ylab = "Density")
abline(v = 0)
```



### Lec12
####root finding
Root Finding bisection example, y is root we need
```{r}
f <- function(y, a, n) {
  a^2 + y^2 + 2*a*y/(n-1) - (n-2)
}
a <- 0.5; n <- 20

# initial interval
b0 <- 0; b1 <- 5*n

#solve using bisection
it <- 0
eps <- .Machine$double.eps^0.25
r <- seq(b0, b1, length=3)
y <- c(f(r[1], a, n), f(r[2], a, n), f(r[3], a, n))
if (y[1] * y[3] > 0)
  stop("f does not have opposite sign at endpoints")

while(it < 1000 & abs(y[2]) > eps) {
  it <- it + 1
  if (y[1]*y[2] < 0) { 
    r[3] <- r[2]; 
    y[3] <- y[2]
  }
  else { 
    r[1] <- r[2]; 
    y[1] <- y[2] 
  }
  r[2] <- (r[1] + r[3]) / 2
  y[2] <- f(r[2], a=a, n=n)
  print(c(r[1], y[1], y[3]-y[2]))
}

```

Root Finding Brent’s example, y is root we need
```{r}
a <- 0.5; n <- 20
out <- uniroot(
  function(y){ a^2 + y^2 + 2*a*y/(n-1) - (n-2) },
  lower = 0, upper = n*5 
)
unlist(out)

```

#### Numerical integration
```{r}
f <- function(y, n, r, rho) { (cosh(y) - rho * r)^(1 - n)
}
integrate(f, lower=0, upper=Inf,
rel.tol=.Machine$double.eps^0.25, n=10, r=0.5, rho=0.2)

```
```{r}
ro <- seq(-.99, .99, .01)
v <- rep(0, length(ro))
for(i in 1:length(ro)) {
  v[i] <- integrate(f, lower=0, upper=Inf,
  rel.tol = .Machine$double.eps^0.25, n=10, r=0.5, rho=ro[i]) $value 
}
plot(ro, v, type="l", xlab=expression(rho),
ylab="Integral Value (n=10, r=0.5)")
```

#### mle method for mle
```{r}
y <- c(0.04304550, 0.50263474)

# log form in mle: l(\theta)
mlogL <- function(theta=1){
#minus log-likelihood of exp. density, rate 1/theta
  return(-(length(y) * log(theta) - theta * sum(y))) 
}

library(stats4)
fit <- mle(mlogL)
summary(fit)
```

constrOptim
```{r}
f <- function(x) log(x + log(x))/log(1+x)
curve(f(x), from = 2, to = 15, ylab = "f(x)")

# find the the maximum w
optimize(f, lower = 4, upper = 8, maximum = TRUE)
```

Maximum Likelihood Estimation with mle by root finding (gamma)
```{r}
m <- 20000
est <- matrix(0, m, 2)  # m * 2
n <- 200
r <- 5
lambda <- 2
obj <- function(lambda, xbar, logx.bar) {
  digamma(lambda * xbar) - logx.bar - log(lambda) 
}
for (i in 1:m) { 
  x <- rgamma(n, shape=r, rate=lambda)
  xbar <- mean(x)
  u <- uniroot(obj, lower = .001, upper = 10e5,
  xbar = xbar, logx.bar = mean(log(x)))
  est[i, ] <- c(xbar* u$root, u$root )
}
ML <- colMeans(est) ; 
print(ML)
```

two dimension optim
```{r}
LL <- function(theta, sx, slogx, n) {
  r <- theta[1]
  lambda <- theta[2]
  loglik <- n * r * log(lambda) + (r - 1) * slogx -
    lambda * sx - n * log(gamma(r)) 
  - loglik
}
r <- 5; lambda <- 2; n <-200;
x <- rgamma(n, shape=r, rate=lambda)
optim(c(1,1), LL, sx=sum(x), slogx=sum(log(x)), n=n)
```
```{r}
mlests <- replicate(20000, expr = {
x <- rgamma(200, shape = 5, rate = 2)
optim(c(1,1), LL, sx=sum(x), slogx=sum(log(x)), n=n)$par
})
colMeans(t(mlests))

```

MLE for a quadratic form
```{r}
LL <- function(lambda, y) {
  lambda3 <- 1 - sum(lambda)
  f1 <- dgamma(y, shape=1/2, rate=1/(2*lambda[1]))
  f2 <- dgamma(y, shape=1/2, rate=1/(2*lambda[2]))
  f3 <- dgamma(y, shape=1/2, rate=1/(2*lambda3))
  
  # have 3 parameters
  f <- f1/3 + f2/3 + f3/3 #density of mixture
  return( -sum(log(f))) #returning -log likelihood
}
set.seed(543)
m <- 2000
lambda <- c(.6, .25, .15) #rate is 1/(2 lambda)
lam <- sample(lambda, size = 2000, replace = TRUE)
y <- rgamma(m, shape = .5, rate = 1/(2*lam))
opt <- optim(c(.5,.3), LL, y=y)
theta <- c(opt$par, 1 - sum(opt$par))
theta
```
