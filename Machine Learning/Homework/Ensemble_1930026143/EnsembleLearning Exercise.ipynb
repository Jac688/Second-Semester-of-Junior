{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS4023 Machine Learning : Ensemble Learning Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise, you'll explore different ensemble methods and how does ensemble improves the performance of a machine learning model. There are three parts in this exercise:\n",
    "1. Simple ensemble strategy: majority voting\n",
    "2. Bagging Method\n",
    "3. Boosting Method: Adaboost\n",
    "\n",
    "The dataset we use for this exercise is a cancer dataset with 699 instances and a total number of 9 features labeled in either benign or malignant classes (0 for benign, 1 for malignant). The dataset only contains numeric values and has been normalized.\n",
    "\n",
    "Many methods will use random generator, e.g., train-test split, decision tree model, bagging boostramp sample generation, therefore, we can set the seed to a fixed number in order to achieve same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.379749</td>\n",
       "      <td>0.237164</td>\n",
       "      <td>0.245271</td>\n",
       "      <td>0.200763</td>\n",
       "      <td>0.246225</td>\n",
       "      <td>0.346352</td>\n",
       "      <td>0.270863</td>\n",
       "      <td>0.207439</td>\n",
       "      <td>0.065490</td>\n",
       "      <td>0.344778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.312860</td>\n",
       "      <td>0.339051</td>\n",
       "      <td>0.330213</td>\n",
       "      <td>0.317264</td>\n",
       "      <td>0.246033</td>\n",
       "      <td>0.364071</td>\n",
       "      <td>0.270929</td>\n",
       "      <td>0.339293</td>\n",
       "      <td>0.190564</td>\n",
       "      <td>0.475636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  \\\n",
       "count       699.000000               699.000000                699.000000   \n",
       "mean          0.379749                 0.237164                  0.245271   \n",
       "std           0.312860                 0.339051                  0.330213   \n",
       "min           0.000000                 0.000000                  0.000000   \n",
       "25%           0.111111                 0.000000                  0.000000   \n",
       "50%           0.333333                 0.000000                  0.000000   \n",
       "75%           0.555556                 0.444444                  0.444444   \n",
       "max           1.000000                 1.000000                  1.000000   \n",
       "\n",
       "       Marginal Adhesion  Single Epithelial Cell Size  Bare Nuclei  \\\n",
       "count         699.000000                   699.000000   699.000000   \n",
       "mean            0.200763                     0.246225     0.346352   \n",
       "std             0.317264                     0.246033     0.364071   \n",
       "min             0.000000                     0.000000     0.000000   \n",
       "25%             0.000000                     0.111111     0.100000   \n",
       "50%             0.000000                     0.111111     0.100000   \n",
       "75%             0.333333                     0.333333     0.500000   \n",
       "max             1.000000                     1.000000     1.000000   \n",
       "\n",
       "       Bland Chromatin  Normal Nucleoli     Mitoses       Class  \n",
       "count       699.000000       699.000000  699.000000  699.000000  \n",
       "mean          0.270863         0.207439    0.065490    0.344778  \n",
       "std           0.270929         0.339293    0.190564    0.475636  \n",
       "min           0.000000         0.000000    0.000000    0.000000  \n",
       "25%           0.111111         0.000000    0.000000    0.000000  \n",
       "50%           0.222222         0.000000    0.000000    0.000000  \n",
       "75%           0.444444         0.333333    0.000000    1.000000  \n",
       "max           1.000000         1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "data = pd.read_csv(\"cancer_normalized.csv\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Simple Ensemble Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at a simple ensemble technique for classification: majority voting. In this method, multiple models are used to make predictions for each data instance. The predictions by each model are considered as a **vote**. The prediction which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "Scikit-Learn provides us with some handy functions that we can use to accomplish this.\n",
    "- The ``VotingClassifier`` takes in a list of different estimators as arguments and a voting method. The ``hard`` voting method uses the predicted labels and a majority rules system, while the ``soft`` voting method predicts a label based on the sum of the predicted probabilities.\n",
    "\n",
    "Here, we use three models, *Decision Tree*, *SVM* and *LogisticRegression*, for voting and adopt 10-fold cross validation. Report the mean accuracy of **individual classifiers and the ensemble by applying the majority voting strategy (**hard voting**).** Compare the performance. \n",
    "- Note: For DecisionTreeClassifier() implementation, the features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9485093167701864 0.9628571428571429 0.9685507246376812 0.9642650103519671\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# your implementation here\n",
    "x = data.iloc[:,:-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=seed)\n",
    "lr_model = LogisticRegression(random_state=seed)\n",
    "svm_model = SVC(random_state=seed)\n",
    "\n",
    "ens_model = VotingClassifier(estimators=[(\"dt_model\", dt_model),\n",
    "                                         (\"lr_model\", lr_model),\n",
    "                                         (\"svc_model\", svm_model)])\n",
    "\n",
    "# Decision Tree, SVM and LogisticRegression performance\n",
    "dt_score = cross_val_score(dt_model, x, y, cv=10).mean()\n",
    "lr_score = cross_val_score(lr_model, x, y, cv=10).mean()\n",
    "svm_score = cross_val_score(svm_model, x, y, cv =10).mean()\n",
    "\n",
    "# voting performance\n",
    "ens_score = cross_val_score(ens_model, x, y,cv=10).mean()\n",
    "\n",
    "print(dt_score, lr_score, svm_score, ens_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore the bagging method by using decision tree as the base learning algorithm. Scikit-Learn provides us a module of ``BaggingClassifier``, we can provide the base learning model and the number of estimation models. Try to set the number of estimators to 100 and report the mean accuracy of the ensemble using 10-fold cross validation. Compare the performance with a single decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9485093167701864 0.9585714285714285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "num_trees = 100\n",
    "\n",
    "# your implemenation here\n",
    "kflow = model_selection.KFold(n_splits=10)\n",
    "bag_model = BaggingClassifier(base_estimator=dt_model,\n",
    "                       n_estimators=num_trees, random_state=seed)\n",
    "# bagging model performance\n",
    "bag_score = cross_val_score(bag_model, x, y, cv=kflow).mean()\n",
    "\n",
    "print(dt_score, bag_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialized a 10-fold cross-validation fold. After that, we instantiated a Decision Tree Classifier with 100 trees and wrapped it in a Bagging-based Ensemble. The accuracy improved to 95.85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn also provides access to the ``RandomForestClassifier``, which is a modification of the decision tree classification. Use random forest model and report the mean accuracy by using 10-folds cross-validation. Number of trees set to 100.\n",
    "\n",
    "**Compare the performance of RandomForestClassifier with bagged decision tree and give the analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9485093167701864 0.9671428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# your implementation here...\n",
    "rf_model = RandomForestClassifier(random_state= seed)\n",
    "rf_score = cross_val_score(rf_model, x, y, cv=10).mean()\n",
    "print(dt_score, rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison and analysis**:\n",
    "\n",
    "...Your analysis here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you use AdaBoost classification by boosting the ``decision stump``(**one-level decision tree**).Try to set the number of rounds to 100 and report the performance of the ensemble. Compare the performance with a single decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9485093167701864 0.9571428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# your implementation here...\n",
    "boost_model = AdaBoostClassifier(n_estimators=100, random_state=seed,\n",
    "                                base_estimator = DecisionTreeClassifier(max_depth=1, random_state=seed))\n",
    "boost_score = cross_val_score(boost_model, x, y, cv=kflow).mean()\n",
    "print(dt_score, boost_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
