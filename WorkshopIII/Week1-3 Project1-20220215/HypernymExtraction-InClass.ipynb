{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypernym Relationship Extraction in Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use NLTK and Hearst Pattern for hypernym relationship extraction. \n",
    "- Firstly, install python environment\n",
    "- Install NLTK: pip install nltk\n",
    "- Download data distribution for NLTK. Install using NLTK downloader: ``nltk.download()``. If cannot download using ``nltk.download()``, try download manually from https://github.com/nltk/nltk_data/tree/gh-pages![image.png](attachment:image.png) or https://pan.baidu.com/s/1wONWpaa86_wnsIksKda8eQ (code:tfon )\n",
    "- Unzip the downloaded file to the following folder: ``nltk.data.find(\".\")``\n",
    "- Unzip each zip file in the ten folders: *chunkers, corpora, grammers, help, misc, models, sentiment, stemmers, taggers, tokenizers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyponym Extraction using Hearst Pattern\n",
    "Hyponym extraction follows the following 4 steps:\n",
    "- Noun phrase chunking or named eneity chunking. You can use any np chunking/named entity technique.\n",
    "- Chunked sentences prepare. Traverse the chunked result, if the label is ``NP``, then merge all the words in this chunk and add a prefix ``NP_`` (for subsequence process).\n",
    "- Chunking refinement. If two or more NPs next to each other should be merged into a single NP. Eg., *\"NP_foo NP_bar blah blah\"* becomes *\"NP_foo_bar blah blah\"*\n",
    "- Find the hypernym and hyponym pairs based on the refined prepared chunked sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import pos_tag, word_tokenize, Tree, ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression practice: In this example, we show one regex pattern example for Hearst pattern: ``NP such as {NP,}* {(or | and)} NP`` (https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_1 such as NP_2 , NP_3 and NP_4 \n"
     ]
    }
   ],
   "source": [
    "# case 1\n",
    "# \"I like to listen to music from musical genres,such as blues,rock and jazz.\"\n",
    "regex = r\"(NP_\\w+ (, )?such as (NP_\\w+ ?(, )?(and |or )?)+)\"\n",
    "test_str = \"NP_1 such as NP_2 , NP_3 and NP_4 \"\n",
    "matches = re.search(regex, test_str)\n",
    "if matches:\n",
    "    # Match.group([group1, ...]) Returns one or more subgroups of the match. \n",
    "    # If there is a single argument, the result is a single string;\n",
    "    # if there are multiple arguments, the result is a tuple with one item per argument. \n",
    "    # Without arguments, group1 defaults to zero (the whole match is returned).\n",
    "    print(matches.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "such NP_1 as NP_2 and NP_3 \n"
     ]
    }
   ],
   "source": [
    "# case 2\n",
    "# \"... works by such authors as Herrick, Goldsmith, and Shakespeare.\"\n",
    "regex = r\"(such NP_\\w+ as (NP_\\w+ ?(, )?(and |or )?)+)\"\n",
    "test_str = \"such NP_1 as NP_2 and NP_3 \"\n",
    "matches = re.search(regex, test_str)\n",
    "if matches:\n",
    "    print(matches.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_1, NP_2 and other NP_3\n"
     ]
    }
   ],
   "source": [
    "# case 3\n",
    "# \"... bistros, coffee shops, and other cheap eating places.\"\n",
    "regex = r\"((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)\"\n",
    "test_str = \"NP_1, NP_2 and other NP_3 \"\n",
    "matches = re.search(regex, test_str)\n",
    "if matches:\n",
    "    print(matches.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_1 including NP_2, NP_3, and NP_4 \n"
     ]
    }
   ],
   "source": [
    "# case 4\n",
    "# \"...all common law countries, including Canada and England.\"\n",
    "regex = r\"(NP_\\w+ ?(, )?including (NP_\\w+ ?(, )?(and |or )?)+)\"\n",
    "test_str = \"NP_1 including NP_2, NP_3, and NP_4 \"\n",
    "matches = re.search(regex, test_str)\n",
    "if matches:\n",
    "    print(matches.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_1 especially NP_2, NP_3, and NP_4 \n"
     ]
    }
   ],
   "source": [
    "# case 5\n",
    "# \"...most European countries, especially France, England, and Spain.\"\n",
    "regex = r\"(NP_\\w+ ?(, )?especially (NP_\\w+ ?(, )?(and |or )?)+)\"\n",
    "test_str = \"NP_1 especially NP_2, NP_3, and NP_4 \"\n",
    "matches = re.search(regex, test_str)\n",
    "if matches:\n",
    "    print(matches.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Chunking Sentence\n",
    "- Note the result is not the chunked np, instead is the chunk tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  listen/VB\n",
      "  to/TO\n",
      "  (NP music/NN)\n",
      "  from/IN\n",
      "  (NP musical/JJ genres/NNS)\n",
      "  ,/,\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  (NP blues/NNS)\n",
      "  ,/,\n",
      "  (NP rock/NN)\n",
      "  and/CC\n",
      "  (NP jazz/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "def np_chunking(sentence):\n",
    "    grammer = \"NP: {<JJ>*<NN.*>+}\\n {<NN.*>+}\"  # chunker finds any number of adjectives (JJ) and then followed by  a nouns (NN)\n",
    "    cp = nltk.RegexpParser(grammer)\n",
    "    result = cp.parse(pos_tag(word_tokenize(sentence))) \n",
    "    #result = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "    return result\n",
    "\n",
    "result_chunks = np_chunking(\"\"\"I like to listen to music from musical genres,such as blues,rock and jazz.\"\"\")\n",
    "result_chunks.draw()\n",
    "print(result_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Prepare the chunked result for subsequent Hearst pattern matching\n",
    "- Traverse the chunked result, if the label is ``NP``, then merge all the words in this chunk and add a prefix ``NP_``\n",
    "- All the tokens are separated with a white space (``\" \"``) \n",
    "- Remember to lemmatize words, using ``WordNetLemmatizer`` (``from nltk.stem import WordNetLemmatizer``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the chunked sentence by merging words and add prefix NP_\n",
    "# this function cannot handle case 2 and 3 correctly, need to be improved\n",
    "def prepare_chunks(chunks):\n",
    "    terms = []\n",
    "    for chunk in chunks:\n",
    "        if type(chunk) == Tree:\n",
    "#             for token, pos in chunk.leaves():\n",
    "#                 tokens.append(token)\n",
    "            # todo: need to handle such and other cases\n",
    "            # NP_such_NEx to such NP_NEx\n",
    "            # NP_other_NEx to other NP_NEx\n",
    "            for a in chunk:\n",
    "                if a[0] == 'other':\n",
    "                    terms.append('other')\n",
    "                if a[0] == 'such':\n",
    "                    terms.append('such')\n",
    "            np = \"NP_\"+\"_\".join([WordNetLemmatizer().lemmatize(a[0]) for a in chunk if a[0]!='other'and a[0]!='such'])\n",
    "            terms.append(np)\n",
    "        else:\n",
    "            terms.append(chunk[0]) \n",
    "    return ' '.join(terms)   # use space to join every term, all the commas will be separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_Agar is a NP_substance prepared from a NP_mixture of NP_red_algae , such as NP_Gelidium , for NP_laboratory or NP_industrial_use .\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"Agar is a substance prepared from a mixture of red algae, such as Gelidium,for laboratory or industrial use.\"\n",
    "chunk_res = np_chunking(raw_text)\n",
    "print(prepare_chunks(chunk_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to listen to NP_music from NP_musical_genre , such as NP_blue , NP_rock and NP_jazz .\n"
     ]
    }
   ],
   "source": [
    "# case 1\n",
    "raw_text = \"I like to listen to music from musical genres,such as blues,rock and jazz.\"\n",
    "chunk_res = np_chunking(raw_text)\n",
    "print(prepare_chunks(chunk_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... NP_work by such NP_author as NP_Herrick , NP_Goldsmith , and NP_Shakespeare .\n"
     ]
    }
   ],
   "source": [
    "# case 2\n",
    "# have a problem, such is an JJ, it is in the NP\n",
    "raw_text = \"... works by such authors as Herrick, Goldsmith, and Shakespeare.\"\n",
    "chunk_res = np_chunking(raw_text)\n",
    "print(prepare_chunks(chunk_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... NP_bistro , NP_coffee_shop , and other NP_cheap_eating_place .\n"
     ]
    }
   ],
   "source": [
    "# case 3\n",
    "# have a problem, other is an JJ, it is in the NP\n",
    "raw_text = \"... bistros, coffee shops, and other cheap eating places.\"\n",
    "chunk_res = np_chunking(raw_text)\n",
    "\n",
    "print(prepare_chunks(chunk_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... all NP_common_law_country , including NP_Canada and NP_England .\n"
     ]
    }
   ],
   "source": [
    "# case 4\n",
    "raw_text = \"...all common law countries, including Canada and England.\"\n",
    "chunk_res = np_chunking(raw_text)\n",
    "print(prepare_chunks(chunk_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... most NP_European_country , especially NP_France , NP_England , and NP_Spain .\n"
     ]
    }
   ],
   "source": [
    "# case 5\n",
    "raw_text = \"...most European countries, especially France, England, and Spain.\"\n",
    "chunk_res = np_chunking(raw_text)\n",
    "print(prepare_chunks(chunk_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Refinement chunking\n",
    "If two or more NPs next to each other should be merged into a single NP. e.g., ``NP_foo NP_bar blah blah`` becomes ``NP_foo_bar blah blah``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_NP(prepared_chunks):\n",
    "    sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\",lambda m: m.expand(r'\\1').replace(\" NP_\", \"_\"),prepared_chunks)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NP_foo_bar blah blah'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_NP(\"NP_foo NP_bar blah blah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'... most NP_European_country , especially NP_France , NP_England , and NP_Spain .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_NP(prepare_chunks(chunk_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Find the hypernym(上位词) and hyponyms(下位词) on processed chunked results\n",
    "- Define Hearst patterns. Besides the regex, we also need to specify whether the hypernym is in the first part or the second part in the pattern.\n",
    "  - For example:\n",
    "  - in the pattern ``NP1 such as NP2 AND NP3``, the hypernym is the first part of the pattern; \n",
    "  - in the pattern ``NP1 , NP2 and other NP3``, the hypernym is the last part of the pattern. \n",
    "- After regex matching, find all the NPs and extract the hypernym and hyponym pairs based on the ``first`` or ``last`` attribute.\n",
    "- Clean the NPs by removing the prefix ``NP_`` and ``_``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given by the prepared text, return the hypernym-hyponym pairs\n",
    "def hyponym_extract(prepared_text, hearst_patterns):\n",
    "    pairs = []\n",
    "    for (pattern,parser) in hearst_patterns:\n",
    "        matches = re.search(pattern, prepared_text)\n",
    "        if matches:\n",
    "            match_str = matches.group(0)\n",
    "            \n",
    "            #find all NP_xx and save to a list\n",
    "            nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "            \n",
    "            if parser == \"first\":\n",
    "                hypernym = nps[0]\n",
    "                hyponyms = nps[1:]\n",
    "            else:\n",
    "                hypernym = nps[-1]\n",
    "                hyponyms = nps[:-1]\n",
    "            for hypo in hyponyms:\n",
    "                pairs.append((hypo,hypernym))\n",
    "    return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NP_blue', 'NP_musical_genre'), ('NP_rock', 'NP_musical_genre'), ('NP_jazz', 'NP_musical_genre')]\n",
      "[('NP_basketball', 'NP_sport'), ('NP_football', 'NP_sport')]\n"
     ]
    }
   ],
   "source": [
    "hearst_patterns = [(\"(NP_\\w+ (, )?such as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                                  (\"((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)\",\"last\")]\n",
    "                   \n",
    "print(hyponym_extract(prepare_chunks(np_chunking(\"I like to listen to music from musical genres,such as blues,rock and jazz.\")),hearst_patterns))\n",
    "print(hyponym_extract(prepare_chunks(np_chunking(\"He likes to play basketball,football and other sports.\")),hearst_patterns))\n",
    "# 理想结果\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
