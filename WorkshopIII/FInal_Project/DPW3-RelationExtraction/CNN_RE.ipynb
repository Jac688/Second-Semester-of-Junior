{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d12312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class WordEmbeddingLoader(object):\n",
    "    def __init__(self):\n",
    "        self.path_word = \"./embedding/hlbl-embeddings-scaled.EMBEDDING_SIZE=50.txt\"  # path of pre-trained word embedding\n",
    "        self.word_dim = 50  # dimension of word embedding\n",
    "\n",
    "    def trim_from_pre_embedding(self, vocab):\n",
    "        word2id = dict()\n",
    "        word_vec = {}\n",
    "        trim_word_vec = list()\n",
    "        with open(self.path_word, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                line = line.strip().split()\n",
    "                if len(line) != self.word_dim + 1:\n",
    "                    continue\n",
    "                word_vec[line[0]] = np.asarray(line[1:], dtype=np.float32)\n",
    "        for word in vocab:\n",
    "            word2id[word] = len(word2id)\n",
    "            if (word in word_vec):\n",
    "                trim_word_vec.append(word_vec[word])\n",
    "            else:\n",
    "                trim_word_vec.append(np.random.uniform(-1, 1, self.word_dim))\n",
    "                \n",
    "        if (\"*UNKNOWN*\" not in word2id):\n",
    "            word2id['*UNKNOWN*'] = len(word2id)\n",
    "            unk_emb = np.random.uniform(-1, 1, self.word_dim)\n",
    "            trim_word_vec.append(unk_emb)\n",
    "        if (\"PAD\" not in word2id):\n",
    "            word2id['PAD'] = len(word2id)\n",
    "            pad_emb = np.zeros(self.word_dim)\n",
    "            trim_word_vec.append(unk_emb)\n",
    "        trim_word_vec = np.array(trim_word_vec)\n",
    "        trim_word_vec = trim_word_vec.astype(np.float32).reshape(-1, self.word_dim)\n",
    "        return word2id, torch.from_numpy(trim_word_vec)\n",
    "\n",
    "    def load_embedding(self):\n",
    "        word2id = dict() \n",
    "        word_vec = list()  \n",
    "        word2id['PAD'] = len(word2id)  # PAD character\n",
    "\n",
    "        with open(self.path_word, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                line = line.strip().split()\n",
    "                if len(line) != self.word_dim + 1:\n",
    "                    continue\n",
    "                word2id[line[0]] = len(word2id)\n",
    "                word_vec.append(np.asarray(line[1:], dtype=np.float32))\n",
    "        if (\"*UNKNOWN*\" not in word2id):\n",
    "            word2id['*UNKNOWN*'] = len(word2id)\n",
    "            unk_emb = np.random.uniform(-1, 1, self.word_dim)\n",
    "            word_vec.append(unk_emb)\n",
    "        pad_emb = np.zeros([1, self.word_dim], dtype=np.float32)  # <pad> is initialize as zero\n",
    "        word_vec = np.concatenate((pad_emb, word_vec), axis=0)\n",
    "        word_vec = word_vec.astype(np.float32).reshape(-1, self.word_dim)\n",
    "        word_vec = torch.from_numpy(word_vec)\n",
    "        return word2id, word_vec\n",
    "\n",
    "\n",
    "class RelationLoader(object):\n",
    "    def __init__(self):\n",
    "        self.data_dir = \"./data\"\n",
    "\n",
    "    def __load_relation(self):\n",
    "        relation_file = os.path.join(self.data_dir, 'relation2id.txt')\n",
    "        rel2id = {}\n",
    "        id2rel = {}\n",
    "        with open(relation_file, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                relation, id_s = line.strip().split()\n",
    "                id_d = int(id_s)\n",
    "                rel2id[relation] = id_d\n",
    "                id2rel[id_d] = relation\n",
    "        return rel2id, id2rel, len(rel2id)\n",
    "\n",
    "    def get_relation(self):\n",
    "        return self.__load_relation()\n",
    "\n",
    "\n",
    "class SemEvalDateset(Dataset):\n",
    "    def __init__(self, filename, rel2id, word2id):\n",
    "        self.filename = filename\n",
    "        self.rel2id = rel2id\n",
    "        self.word2id = word2id\n",
    "        self.max_len = 96\n",
    "        self.pos_dis = 20\n",
    "        self.data_dir = \"./data\"\n",
    "        self.dataset, self.label = self.__load_data()\n",
    "\n",
    "    def __get_pos_index(self, x):\n",
    "        if x < -self.pos_dis:\n",
    "            return 0\n",
    "        if x >= -self.pos_dis and x <= self.pos_dis:\n",
    "            return x + self.pos_dis + 1\n",
    "        if x > self.pos_dis:\n",
    "            return 2 * self.pos_dis + 2\n",
    "\n",
    "    def __get_relative_pos(self, x, entity_pos):\n",
    "        if x < entity_pos[0]:\n",
    "            return self.__get_pos_index(x - entity_pos[0])\n",
    "        elif x > entity_pos[1]:\n",
    "            return self.__get_pos_index(x - entity_pos[1])\n",
    "        else:\n",
    "            return self.__get_pos_index(0)\n",
    "\n",
    "    def __symbolize_sentence(self, e1_pos, e2_pos, sentence):\n",
    "        mask = [1] * len(sentence)\n",
    "        if e1_pos[0] < e2_pos[0]:\n",
    "            for i in range(e1_pos[0], e2_pos[1] + 1):\n",
    "                mask[i] = 2\n",
    "            for i in range(e2_pos[1] + 1, len(sentence)):\n",
    "                mask[i] = 3\n",
    "        else:\n",
    "            for i in range(e2_pos[0], e1_pos[1] + 1):\n",
    "                mask[i] = 2\n",
    "            for i in range(e1_pos[1] + 1, len(sentence)):\n",
    "                mask[i] = 3\n",
    "\n",
    "        words = []\n",
    "        pos1 = []\n",
    "        pos2 = []\n",
    "        length = min(self.max_len, len(sentence))\n",
    "        mask = mask[:length]\n",
    "\n",
    "        for i in range(length):\n",
    "            words.append(self.word2id.get(sentence[i], self.word2id['*UNKNOWN*']))\n",
    "            pos1.append(self.__get_relative_pos(i, e1_pos))\n",
    "            pos2.append(self.__get_relative_pos(i, e2_pos))\n",
    "\n",
    "        if length < self.max_len:\n",
    "            for i in range(length, self.max_len):\n",
    "                mask.append(0)  # 'PAD' mask is zero\n",
    "                words.append(self.word2id['PAD'])\n",
    "\n",
    "                pos1.append(self.__get_relative_pos(i, e1_pos))\n",
    "                pos2.append(self.__get_relative_pos(i, e2_pos))\n",
    "        unit = np.asarray([words, pos1, pos2, mask], dtype=np.int64)\n",
    "        unit = np.reshape(unit, newshape=(1, 4, self.max_len))\n",
    "        return unit\n",
    "\n",
    "    def _lexical_feature(self, e1_idx, e2_idx, sent):\n",
    "        def _entity_context(e_idx, sent):\n",
    "            ''' return [w(e-1), w(e), w(e+1)]\n",
    "            '''\n",
    "            context = []\n",
    "            context.append(sent[e_idx])\n",
    "            if e_idx >= 1:\n",
    "                context.append(sent[e_idx - 1])\n",
    "            else:\n",
    "                context.append(sent[e_idx])\n",
    "\n",
    "            if e_idx < len(sent) - 1:\n",
    "                context.append(sent[e_idx + 1])\n",
    "            else:\n",
    "                context.append(sent[e_idx])\n",
    "            return context\n",
    "\n",
    "        context1 = _entity_context(e1_idx[0], sent)\n",
    "        context2 = _entity_context(e2_idx[0], sent)\n",
    "        lexical = context1 + context2\n",
    "        lexical_ids = [self.word2id.get(word, self.word2id['*UNKNOWN*']) for word in lexical]\n",
    "        lexical_ids = np.asarray(lexical_ids, dtype=np.int64)\n",
    "        return np.reshape(lexical_ids, newshape=(1, 6))\n",
    "\n",
    "    def __load_data(self):\n",
    "        path_data_file = os.path.join(self.data_dir, self.filename)\n",
    "        data = []\n",
    "        labels = []\n",
    "        with open(path_data_file, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                line = json.loads(line.strip())\n",
    "                label = line['relation']\n",
    "                sentence = line['sentence']\n",
    "                e1_pos = (line['subj_start'], line['subj_end'])\n",
    "                e2_pos = (line['obj_start'], line['obj_end'])\n",
    "                label_idx = self.rel2id[label]\n",
    "\n",
    "                one_sentence = self.__symbolize_sentence(e1_pos, e2_pos, sentence)\n",
    "                lexical = self._lexical_feature(e1_pos, e2_pos, sentence)\n",
    "                temp = (one_sentence, lexical)\n",
    "                data.append(temp)\n",
    "                # data.append(one_sentence)\n",
    "                labels.append(label_idx)\n",
    "        return data, labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "\n",
    "class SemEvalDataLoader(object):\n",
    "    def __init__(self, rel2id, word2id):\n",
    "        self.rel2id = rel2id\n",
    "        self.word2id = word2id\n",
    "\n",
    "    def __collate_fn(self, batch):\n",
    "        data, label = zip(*batch) \n",
    "        data = list(data)\n",
    "        label = list(label)\n",
    "        sentence_feat = torch.from_numpy(np.concatenate([x[0] for x in data], axis=0))\n",
    "        lexical_feat = torch.from_numpy(np.concatenate([x[1] for x in data], axis=0))\n",
    "        label = torch.from_numpy(np.asarray(label, dtype=np.int64))\n",
    "        return (sentence_feat, lexical_feat), label\n",
    "\n",
    "    def __get_data(self, filename, shuffle=False):\n",
    "        dataset = SemEvalDateset(filename, self.rel2id, self.word2id)\n",
    "        loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=2,\n",
    "            collate_fn=self.__collate_fn\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def get_train(self):\n",
    "        return self.__get_data('train.json', shuffle=True)\n",
    "\n",
    "    def get_dev(self):\n",
    "        return self.__get_data('test.json', shuffle=False)\n",
    "\n",
    "    def get_test(self):\n",
    "        return self.__get_data('test.json', shuffle=False)\n",
    "\n",
    "\n",
    "class processor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def search_entity(self, sentence):\n",
    "        e1 = re.findall(r'<e1>(.*)</e1>', sentence)[0]\n",
    "        e2 = re.findall(r'<e2>(.*)</e2>', sentence)[0]\n",
    "        sentence = sentence.replace('<e1>' + e1 + '</e1>', ' <e1> ' + e1 + ' </e1> ', 1)\n",
    "        sentence = sentence.replace('<e2>' + e2 + '</e2>', ' <e2> ' + e2 + ' </e2> ', 1)\n",
    "        sentence = word_tokenize(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.replace('< e1 >', '<e1>')\n",
    "        sentence = sentence.replace('< e2 >', '<e2>')\n",
    "        sentence = sentence.replace('< /e1 >', '</e1>')\n",
    "        sentence = sentence.replace('< /e2 >', '</e2>')\n",
    "        sentence = sentence.split()\n",
    "\n",
    "        assert '<e1>' in sentence\n",
    "        assert '<e2>' in sentence\n",
    "        assert '</e1>' in sentence\n",
    "        assert '</e2>' in sentence\n",
    "\n",
    "        subj_start = subj_end = obj_start = obj_end = 0\n",
    "        pure_sentence = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            if '<e1>' == word:\n",
    "                subj_start = len(pure_sentence)\n",
    "                continue\n",
    "            if '</e1>' == word:\n",
    "                subj_end = len(pure_sentence) - 1\n",
    "                continue\n",
    "            if '<e2>' == word:\n",
    "                obj_start = len(pure_sentence)\n",
    "                continue\n",
    "            if '</e2>' == word:\n",
    "                obj_end = len(pure_sentence) - 1\n",
    "                continue\n",
    "            pure_sentence.append(word)\n",
    "        return e1, e2, subj_start, subj_end, obj_start, obj_end, pure_sentence\n",
    "\n",
    "    def convert(self, path_src, path_des):\n",
    "        with open(path_src, 'r', encoding='utf-8') as fr:\n",
    "            data = fr.readlines()\n",
    "        with open(path_des, 'w', encoding='utf-8') as fw:\n",
    "            for i in range(0, len(data), 4):\n",
    "                id_s, sentence = data[i].strip().split('\\t')\n",
    "                sentence = sentence[1:-1]\n",
    "                e1, e2, subj_start, subj_end, obj_start, obj_end, sentence = self.search_entity(sentence)\n",
    "                meta1 = dict(\n",
    "                    id=id_s,\n",
    "                    relation=data[i + 1].strip(),\n",
    "                    head=e1,\n",
    "                    tail=e2,\n",
    "                    subj_start=subj_start,\n",
    "                    subj_end=subj_end,\n",
    "                    obj_start=obj_start,\n",
    "                    obj_end=obj_end,\n",
    "                    sentence=sentence,\n",
    "                    comment=data[i + 2].strip()[8:]\n",
    "                )\n",
    "                json.dump(meta1, fw, ensure_ascii=False)\n",
    "                fw.write('\\n')\n",
    "\n",
    "class VocabGenerator(object):\n",
    "    def __init__(self, train_path, test_path):\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "\n",
    "    def get_vocab(self):\n",
    "        vocab = {}\n",
    "        with open(self.train_path, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                line = json.loads(line.strip())\n",
    "                sentence = line['sentence']\n",
    "                for word in sentence:\n",
    "                    vocab[word] = 1\n",
    "        with open(self.test_path, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                line = json.loads(line.strip())\n",
    "                sentence = line['sentence']\n",
    "                for word in sentence:\n",
    "                    vocab[word] = 1\n",
    "        return vocab\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path_train = './data/TRAIN_FILE.TXT'\n",
    "    path_test = './data/FULL_TEST.txt'\n",
    "    processor1 = processor()\n",
    "    processor1.convert(path_train, './data/train.json')\n",
    "    processor1.convert(path_test, './data/test.json')\n",
    "    vocab = VocabGenerator('./data/train.json', './data/test.json').get_vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2614014a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Other': 0,\n",
       " 'Cause-Effect(e1,e2)': 1,\n",
       " 'Cause-Effect(e2,e1)': 2,\n",
       " 'Component-Whole(e1,e2)': 3,\n",
       " 'Component-Whole(e2,e1)': 4,\n",
       " 'Content-Container(e1,e2)': 5,\n",
       " 'Content-Container(e2,e1)': 6,\n",
       " 'Entity-Destination(e1,e2)': 7,\n",
       " 'Entity-Destination(e2,e1)': 8,\n",
       " 'Entity-Origin(e1,e2)': 9,\n",
       " 'Entity-Origin(e2,e1)': 10,\n",
       " 'Instrument-Agency(e1,e2)': 11,\n",
       " 'Instrument-Agency(e2,e1)': 12,\n",
       " 'Member-Collection(e1,e2)': 13,\n",
       " 'Member-Collection(e2,e1)': 14,\n",
       " 'Message-Topic(e1,e2)': 15,\n",
       " 'Message-Topic(e2,e1)': 16,\n",
       " 'Product-Producer(e1,e2)': 17,\n",
       " 'Product-Producer(e2,e1)': 18}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id, word_vec = WordEmbeddingLoader().load_embedding()\n",
    "rel2id, id2rel, class_num = RelationLoader().get_relation()\n",
    "rel2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab34aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([246123, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae41841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, word_vec, class_num):\n",
    "        super().__init__()\n",
    "        self.word_vec = word_vec\n",
    "        self.class_num = class_num\n",
    "\n",
    "        self.max_len = 96\n",
    "        self.word_dim = 50\n",
    "        self.pos_dim = 5\n",
    "        self.pos_dis = 20\n",
    "\n",
    "        self.dropout_value = 0.5\n",
    "        self.filter_num = 200\n",
    "        self.window = 3\n",
    "        self.hidden_size = 100\n",
    "\n",
    "        self.dim = self.word_dim + 2 * self.pos_dim\n",
    "\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(embeddings=self.word_vec, freeze=False, )\n",
    "        self.pos1_embedding = nn.Embedding(num_embeddings=2 * self.pos_dis + 3, embedding_dim=self.pos_dim)\n",
    "        self.pos2_embedding = nn.Embedding(num_embeddings=2 * self.pos_dis + 3, embedding_dim=self.pos_dim)\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=self.filter_num,\n",
    "            kernel_size=(self.window, self.dim),\n",
    "            stride=(1, 1),\n",
    "            bias=False,\n",
    "            padding=(1, 0), \n",
    "            padding_mode='zeros'\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d((self.max_len, 1))\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(self.dropout_value)\n",
    "        self.linear = nn.Linear(in_features=self.filter_num, out_features=self.hidden_size, bias=False)\n",
    "        self.dense = nn.Linear(in_features=self.hidden_size + 6 * self.word_dim, out_features=self.class_num, bias=False)\n",
    "\n",
    "\n",
    "        init.xavier_normal_(self.pos1_embedding.weight)\n",
    "        init.xavier_normal_(self.pos2_embedding.weight)\n",
    "        init.xavier_normal_(self.conv.weight)\n",
    "        init.xavier_normal_(self.linear.weight)\n",
    "        init.xavier_normal_(self.dense.weight)\n",
    "\n",
    "\n",
    "    def encoder_layer(self, token, pos1, pos2):\n",
    "        word_emb = self.word_embedding(token)  \n",
    "        pos1_emb = self.pos1_embedding(pos1)  \n",
    "        pos2_emb = self.pos2_embedding(pos2)  \n",
    "        emb = torch.cat(tensors=[word_emb, pos1_emb, pos2_emb], dim=-1)\n",
    "        return emb\n",
    "\n",
    "    def conv_layer(self, emb, mask):\n",
    "        emb = emb.unsqueeze(dim=1) \n",
    "        conv = self.conv(emb)  \n",
    "\n",
    "        conv = conv.view(-1, self.filter_num, self.max_len)  \n",
    "        mask = mask.unsqueeze(dim=1)  \n",
    "        mask = mask.expand(-1, self.filter_num, -1) \n",
    "        conv = conv.masked_fill_(mask.eq(0), float('-inf'))  \n",
    "        conv = conv.unsqueeze(dim=-1) \n",
    "        return conv\n",
    "\n",
    "    def single_maxpool_layer(self, conv):\n",
    "        pool = self.maxpool(conv)  \n",
    "        pool = pool.view(-1, self.filter_num)  \n",
    "        return pool\n",
    "\n",
    "    def forward(self, data):\n",
    "        token = data[0][:, 0, :].view(-1, self.max_len)\n",
    "        pos1 = data[0][:, 1, :].view(-1, self.max_len)\n",
    "        pos2 = data[0][:, 2, :].view(-1, self.max_len)\n",
    "        mask = data[0][:, 3, :].view(-1, self.max_len)\n",
    "        lexical = data[1].view(-1, 6)\n",
    "        lexical_emb = self.word_embedding(lexical)\n",
    "        lexical_emb = lexical_emb.view(-1, self.word_dim * 6)\n",
    "        emb = self.encoder_layer(token, pos1, pos2)\n",
    "        emb = self.dropout(emb)\n",
    "        conv = self.conv_layer(emb, mask)\n",
    "        pool = self.single_maxpool_layer(conv)\n",
    "        sentence_feature = self.linear(pool)\n",
    "        sentence_feature = self.tanh(sentence_feature)\n",
    "        sentence_feature = self.dropout(sentence_feature)\n",
    "        features = torch.cat((lexical_emb, sentence_feature), 1)\n",
    "        logits = self.dense(features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c824fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eval(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, model, criterion, data_loader):\n",
    "        predict_label = []\n",
    "        true_label = []\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for _, (data, label) in enumerate(data_loader):\n",
    "                sent_feat = data[0]\n",
    "                lex_feat = data[1]\n",
    "                data = (sent_feat, lex_feat)\n",
    "                label = label\n",
    "\n",
    "                scores = model(data)\n",
    "                loss = criterion(scores, label)\n",
    "                total_loss += loss.item() * scores.shape[0]\n",
    "\n",
    "                scores, pred = torch.max(scores[:, 1:], dim=1)\n",
    "                pred = pred + 1\n",
    "\n",
    "                scores = scores.cpu().detach().numpy().reshape((-1, 1))\n",
    "                pred = pred.cpu().detach().numpy().reshape((-1, 1))\n",
    "                label = label.cpu().detach().numpy().reshape((-1, 1))\n",
    "\n",
    "                for i in range(pred.shape[0]):\n",
    "                    if scores[i][0] < 0:\n",
    "                        pred[i][0] = 0\n",
    "\n",
    "                predict_label.append(pred)\n",
    "                true_label.append(label)\n",
    "        predict_label = np.concatenate(predict_label, axis=0).reshape(-1).astype(np.int64)\n",
    "        true_label = np.concatenate(true_label, axis=0).reshape(-1).astype(np.int64)\n",
    "        eval_loss = total_loss / predict_label.shape[0]\n",
    "\n",
    "        f1 = semeval_scorer(predict_label, true_label)\n",
    "        return f1, eval_loss, predict_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b171d537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!\n",
      "--------------------------------------\n",
      "CNN()\n",
      "(word_embedding): Embedding(246123, 50)\n",
      "(pos1_embedding): Embedding(103, 5)\n",
      "(pos2_embedding): Embedding(103, 5)\n",
      "(conv): Conv2d(1, 200, kernel_size=(3, 60), stride=(1, 1), padding=(1, 0))\n",
      "(maxpool): MaxPool2d(kernel_size=(100, 1), stride=(100, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "(tanh): Tanh()\n",
      "(dropout): Dropout(p=0.5, inplace=False)\n",
      "(linear): Linear(in_features=200, out_features=100, bias=True)\n",
      "(dense): Linear(in_features=100, out_features=19, bias=True)\n",
      "traning model parameters:\n",
      "word_embedding.weight :  torch.Size([246123, 50])\n",
      "pos1_embedding.weight :  torch.Size([103, 5])\n",
      "pos2_embedding.weight :  torch.Size([103, 5])\n",
      "conv.weight :  torch.Size([200, 1, 3, 60])\n",
      "conv.bias :  torch.Size([200])\n",
      "linear.weight :  torch.Size([100, 200])\n",
      "linear.bias :  torch.Size([100])\n",
      "dense.weight :  torch.Size([19, 100])\n",
      "dense.bias :  torch.Size([19])\n",
      "--------------------------------------\n",
      "start to train the model ...\n",
      "[001] train_loss: 2.446 | dev_loss: 2.449 | micro f1 on dev: 0.5220\n",
      "[002] train_loss: 1.856 | dev_loss: 1.850 | micro f1 on dev: 0.4652\n",
      "[003] train_loss: 1.403 | dev_loss: 1.426 | micro f1 on dev: 0.5612\n",
      "[004] train_loss: 1.186 | dev_loss: 1.265 | micro f1 on dev: 0.6535\n",
      "[005] train_loss: 0.976 | dev_loss: 1.131 | micro f1 on dev: 0.6994\n",
      "[006] train_loss: 0.806 | dev_loss: 1.039 | micro f1 on dev: 0.7259\n",
      "[007] train_loss: 0.658 | dev_loss: 0.976 | micro f1 on dev: 0.7508\n",
      "[008] train_loss: 0.571 | dev_loss: 0.972 | micro f1 on dev: 0.7557\n",
      "[009] train_loss: 0.457 | dev_loss: 0.927 | micro f1 on dev: 0.7690\n",
      "[010] train_loss: 0.364 | dev_loss: 0.905 | micro f1 on dev: 0.7666\n",
      "[011] train_loss: 0.289 | dev_loss: 0.896 | micro f1 on dev: 0.7796\n",
      "[012] train_loss: 0.234 | dev_loss: 0.912 | micro f1 on dev: 0.7846\n",
      "[013] train_loss: 0.189 | dev_loss: 0.916 | micro f1 on dev: 0.7926\n",
      "[014] train_loss: 0.151 | dev_loss: 0.924 | micro f1 on dev: 0.7857\n",
      "[015] train_loss: 0.123 | dev_loss: 0.959 | micro f1 on dev: 0.7862\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def print_result(predict_label, id2rel, start_idx=8001):\n",
    "    with open('script/predicted_result.txt', 'w', encoding='utf-8') as fw:\n",
    "        for i in range(0, predict_label.shape[0]):\n",
    "            fw.write('{}\\t{}\\n'.format(start_idx + i, id2rel[int(predict_label[i])]))\n",
    "\n",
    "\n",
    "def train(model, criterion, loader):\n",
    "    train_loader, dev_loader, _ = loader\n",
    "    print(loader)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "    print(model)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print('%s :  %s' % (name, str(param.data.shape)))\n",
    "    print('--------------------------------------')\n",
    "    print('start to train the model ...')\n",
    "\n",
    "    eval_tool = Eval()\n",
    "    min_f1 = -float('inf')\n",
    "    for epoch in range(1, 100 + 1):\n",
    "        for step, (data, label) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            \n",
    "\n",
    "            sent_feat = data[0]\n",
    "            lex_feat = data[1]\n",
    "            data = (sent_feat, lex_feat)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _, train_loss, _ = eval_tool.evaluate(model, criterion, train_loader)\n",
    "        f1, dev_loss, _ = eval_tool.evaluate(model, criterion, dev_loader)\n",
    "\n",
    "        print('[%03d] train_loss: %.3f | dev_loss: %.3f | micro f1 on dev: %.4f' % (epoch, train_loss, dev_loss, f1), end=' ')\n",
    "        if f1 > min_f1:\n",
    "            min_f1 = f1\n",
    "        \n",
    "            model_dir = os.path.join(\"./model\")\n",
    "            if not os.path.exists(model_dir):\n",
    "                os.makedirs(model_dir)\n",
    "            torch.save(model.state_dict(), os.path.join(model_dir, 'model.pkl'))\n",
    "\n",
    "\n",
    "def test(model, criterion, loader):\n",
    "    print('--------------------------------------')\n",
    "    print('start test ...')\n",
    "    _, _, test_loader = loader\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pkl')))\n",
    "    eval_tool = Eval()\n",
    "    f1, test_loss, predict_label = eval_tool.evaluate(model, criterion, test_loader)\n",
    "    print('test_loss: %.3f | micro f1 on test:  %.4f' % (test_loss, f1))\n",
    "    return predict_label\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    vocab = VocabGenerator('data/train.json', 'data/test.json').get_vocab()\n",
    "    word2id, word_vec = WordEmbeddingLoader().load_embedding()\n",
    "    rel2id, id2rel, class_num = RelationLoader().get_relation()\n",
    "    loader = SemEvalDataLoader(rel2id, word2id )\n",
    "\n",
    "    train_loader, dev_loader = None, None\n",
    "    train_loader = loader.get_train()\n",
    "    dev_loader = loader.get_dev()\n",
    "    test_loader = loader.get_test()\n",
    "    loader = [train_loader, dev_loader, test_loader]\n",
    "    print('finish!')\n",
    "\n",
    "    print('--------------------------------------')\n",
    "    model = CNN(word_vec=word_vec, class_num=class_num)\n",
    "    \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    train(model, criterion, loader)\n",
    "    predict_label = test(model, criterion, loader)\n",
    "    print_result(predict_label, id2rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb63c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf354db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
